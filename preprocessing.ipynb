{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26953a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Table from CSV\n",
    "\n",
    "# Parameters\n",
    "file_path = 'data/SampleRoadAcceleration.csv'\n",
    "table_marker = 'Line'\n",
    "target_header_num = 3 # Should be 1 or more\n",
    "total_headers_num = 4\n",
    "limit_lines_search = 300 \n",
    "\n",
    "# Loop\n",
    "flag_marker = False\n",
    "with open(file_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if (table_marker in line) and not flag_marker:\n",
    "            # Assignation\n",
    "            first_header_row = i\n",
    "            target_header_row = i + target_header_num - 1\n",
    "            table_row = i + total_headers_num\n",
    "            flag_marker = True\n",
    "\n",
    "        if i == target_header_row:\n",
    "            # Header row extraction\n",
    "            columns = [col.strip() for col in line.split(',')]\n",
    "            break\n",
    "\n",
    "        if i > limit_lines_search:\n",
    "            raise ValueError(f\"Marker '{table_marker}' not found within the first {limit_lines_search} lines of the file.\")\n",
    "\n",
    "# DataFrame\n",
    "df = pd.read_csv(file_path, skiprows=table_row, header=None, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "87a6eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient, vectorized payload extraction for up to 64 bytes\n",
    "\n",
    "# Parameters\n",
    "extended_payload_cols = [f'B{i}' for i in range(1, 65)]\n",
    "current_payload_cols = [f'B{i}' for i in range(1, 9)]\n",
    "\n",
    "# Identify rows with long payloads (space in B1)\n",
    "long_payload_mask = df['B1'].astype(str).str.contains(' ')\n",
    "\n",
    "# Split long payloads into bytes (vectorized) and rename columns to B1, B2, ..., B64\n",
    "long_payload_bytes = (\n",
    "    df.loc[long_payload_mask, 'B1']\n",
    "    .str.strip()\n",
    "    .str.split(' ', expand=True)\n",
    ")\n",
    "long_payload_bytes.columns = extended_payload_cols[:long_payload_bytes.shape[1]]\n",
    "\n",
    "# For short payloads, stack B1-B8 columns as strings (vectorized)\n",
    "short_payload_bytes = (\n",
    "    df.loc[~long_payload_mask, current_payload_cols]\n",
    ")\n",
    "\n",
    "# Pad both to 64 columns\n",
    "def pad_to_64(df_bytes):\n",
    "    cols = [f'B{i}' for i in range(1, 65)]\n",
    "    df_bytes = df_bytes.reindex(columns=cols, fill_value=np.nan)\n",
    "    return df_bytes\n",
    "\n",
    "long_payload_bytes = pad_to_64(long_payload_bytes)\n",
    "short_payload_bytes = pad_to_64(short_payload_bytes)\n",
    "\n",
    "# Combine back into one DataFrame, preserving original order\n",
    "payload_df = pd.concat([long_payload_bytes, short_payload_bytes]).sort_index()\n",
    "\n",
    "# Reorganize columns to match the original payload structure\n",
    "first_payload_idx = df.columns.get_loc(current_payload_cols[0])\n",
    "new_col_order = (\n",
    "    list(df.columns[:first_payload_idx]) +\n",
    "    extended_payload_cols +\n",
    "    list(df.columns[first_payload_idx + len(current_payload_cols):])\n",
    ")\n",
    "# Create the extended dataframe\n",
    "df_extended = pd.concat([df.drop(columns=current_payload_cols), payload_df], axis=1)\n",
    "df_extended = df_extended[new_col_order]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "34be6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns of interest, prepare for export\n",
    "target_columns = ['Abs Time(Sec)', 'Network', 'Arb ID'] + extended_payload_cols\n",
    "# Rename columns dict\n",
    "rename_dict = {'Abs Time(Sec)': 'abs_time',\n",
    "               'Network': 'network',\n",
    "               'Arb ID': 'arb_id'\n",
    "               }\n",
    "\n",
    "# Create a list of numeric column names for payload bytes (as strings)\n",
    "numeric_payload_cols = [str(i) for i in range(1, len(extended_payload_cols) + 1)]\n",
    "rename_dict.update(\n",
    "    dict(zip(extended_payload_cols, numeric_payload_cols))\n",
    ")\n",
    "\n",
    "\n",
    "# Create df_hex with renamed columns\n",
    "df_hex = df_extended[target_columns].rename(columns=rename_dict)\n",
    "\n",
    "\n",
    "# Add a 'length' column before 'arb_id' indicating the number of non-NaN payload bytes for each row\n",
    "df_hex.insert(df_hex.columns.get_loc('arb_id'), 'length', df_hex[numeric_payload_cols].notna().sum(axis=1).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e78cba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to int: All payload columns from hex string to integer. Preserve NaN, type of columns should be uint8\n",
    "df_int = df_hex.copy()\n",
    "df_int[numeric_payload_cols] = (\n",
    "    df_hex[numeric_payload_cols]\n",
    "    .map(lambda x: int(x, 16) if pd.notna(x) else np.nan)\n",
    "    .astype('UInt8')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f434f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export\n",
    "\n",
    "# Parameters\n",
    "export_file_path = 'data/SampleRoadAcceleration_All.csv'\n",
    "\n",
    "df_export = df_int.copy()\n",
    "df_export.to_csv(export_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
